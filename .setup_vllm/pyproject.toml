[build-system]
requires = ['setuptools >= 61.0']
build-backend = 'setuptools.build_meta'

[project]
name = 'CLaRA_v1'
dynamic = ['version']
authors = [{name='Pierre-Olivier Bonin'},
           {name='Marc-Andr√© Allard'}]
description = 'RAG-powered AI Research Assistant leveraging a ChromaDB vector database, a Streamlit frontend and a vLLM backend.'
readme = 'README.md'
requires-python = '== 3.12.*'
dependencies = [
    'flashinfer-python==0.2.5',
    'llmcompressor==0.5.1',
    'ollama==0.5.1',
    'protobuf==4.25.8',
    'streamlit==1.45.1',
    # vllm will install torch, and it will detect CUDA on its own. CUDA drivers compatibility is a bit finnicky; consult https://docs.flashinfer.ai/installation.html
    "vllm==0.9.0.1",
    ]

[tool.setuptools.packages.find]
where = ["../src/"]

[tool.uv.sources]
torch = { index = "pytorch-cu124" }

[[tool.uv.index]]
name = "pytorch-cu124"
url = "https://download.pytorch.org/whl/cu124"
explicit = true
